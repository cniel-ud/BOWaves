{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "This notebook is for running a first try of BOWave on the Frolich et. al data to see if we match their paper's results.\n",
    "This requires 16 < x < 32 gb of RAM. Recommend running on Caviness with --mem-per-cpu=32gb flag set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#Load ICs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Obtaining dependency information for matplotlib from https://files.pythonhosted.org/packages/c2/da/a5622266952ab05dc3995d77689cba600e49ea9d6c51d469c077695cb719/matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Obtaining dependency information for contourpy>=1.0.1 from https://files.pythonhosted.org/packages/aa/55/02c6d24804592b862b38a85c9b3283edc245081390a520ccd11697b6b24f/contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Obtaining dependency information for fonttools>=4.22.0 from https://files.pythonhosted.org/packages/2b/e8/61b8525acf26ec222518bdff127ae502bfa3408981fb5e5493f2b037d7fb/fonttools-4.42.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading fonttools-4.42.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (150 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m151.0/151.0 kB\u001B[0m \u001B[31m11.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Obtaining dependency information for kiwisolver>=1.0.1 from https://files.pythonhosted.org/packages/6f/40/4ab1fdb57fced80ce5903f04ae1aed7c1d5939dda4fd0c0aa526c12fe28a/kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in /work/cniel/ajmeek/BOWaves/venv/lib/python3.10/site-packages (from matplotlib) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /work/cniel/ajmeek/BOWaves/venv/lib/python3.10/site-packages (from matplotlib) (23.1)\n",
      "Collecting pillow>=6.2.0 (from matplotlib)\n",
      "  Obtaining dependency information for pillow>=6.2.0 from https://files.pythonhosted.org/packages/7b/c9/08de9a629ce7cdeaea0ddca716e9efcd1844b2650f5b9dd8ec5609e40ffe/Pillow-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading Pillow-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting pyparsing<3.1,>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /work/cniel/ajmeek/BOWaves/venv/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /work/cniel/ajmeek/BOWaves/venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.6/11.6 MB\u001B[0m \u001B[31m48.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m0:01\u001B[0m\n",
      "\u001B[?25hDownloading contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m300.7/300.7 kB\u001B[0m \u001B[31m24.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading fonttools-4.42.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.5/4.5 MB\u001B[0m \u001B[31m51.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m\n",
      "\u001B[?25hDownloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m53.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading Pillow-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.3/3.3 MB\u001B[0m \u001B[31m51.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.1.0 cycler-0.11.0 fonttools-4.42.1 kiwisolver-1.4.5 matplotlib-3.7.2 pillow-10.0.0 pyparsing-3.0.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /work/cniel/ajmeek/BOWaves/venv/lib/python3.10/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /work/cniel/ajmeek/BOWaves/venv/lib/python3.10/site-packages (from scikit-learn) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /work/cniel/ajmeek/BOWaves/venv/lib/python3.10/site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /work/cniel/ajmeek/BOWaves/venv/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /work/cniel/ajmeek/BOWaves/venv/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pyrootutils\n",
    "\n",
    "pyrootutils.set_root(path='/work/cniel/ajmeek/BOWaves/BOWaves', pythonpath=True)\n",
    "#path = pyrootutils.find_root(search_from=__file__, indicator=\".git\")\n",
    "#pyrootutils.set_root(path=path, pythonpath=True)\n",
    "\n",
    "import BOWaves.utilities.dataloaders as dataloaders\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#frolich_ics = {'ICs': np.array([]), 'labels': np.array([])}\n",
    "frolich_ics = {'ICs': [], 'labels': []}\n",
    "\n",
    "#for file in directory frolich data\n",
    "frolich_data = os.listdir('../data/frolich')\n",
    "\n",
    "#filter out subdirectories such as /img\n",
    "frolich_data = [file for file in frolich_data if not os.path.isdir(file)]\n",
    "\n",
    "for file in frolich_data:\n",
    "    ICs, labels = dataloaders.load_and_visualize_mat_file_frolich('../data/frolich/' + file, visualize=False)\n",
    "    frolich_ics['ICs'].extend(ICs)\n",
    "    frolich_ics['labels'].extend(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now create codebooks since we have the ICs and their labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "First split off 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(frolich_ics['ICs'], frolich_ics['labels'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now out of the training set, split into the different classes. Frolich's data has 4 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if len(X_train) != len(y_train):\n",
    "    raise ValueError('X_train and y_train are not the same length.')\n",
    "\n",
    "all_classes = ['neural', 'blink', 'muscle', 'mixed', 'lateyes', 'heart']\n",
    "\n",
    "# Forgot what the classes were. check on Caviness\n",
    "neural = {'ICs': [], 'centroids': [], 'labels': [], 'shifts': [], 'distances': [], 'inertia': []}\n",
    "blink = {'ICs': [], 'centroids': [], 'labels': [], 'shifts': [], 'distances': [], 'inertia': []}\n",
    "muscle = {'ICs': [], 'centroids': [], 'labels': [], 'shifts': [], 'distances': [], 'inertia': []}\n",
    "mixed = {'ICs': [], 'centroids': [], 'labels': [], 'shifts': [], 'distances': [], 'inertia': []}\n",
    "lateyes = {'ICs': [], 'centroids': [], 'labels': [], 'shifts': [], 'distances': [], 'inertia': []}\n",
    "heart = {'ICs': [], 'centroids': [], 'labels': [], 'shifts': [], 'distances': [], 'inertia': []}\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    if y_train[i] == 'neural':\n",
    "        neural['ICs'].append(X_train[i])\n",
    "    elif y_train[i] == 'blink':\n",
    "        blink['ICs'].append(X_train[i])\n",
    "    elif y_train[i] == 'muscle':\n",
    "        muscle['ICs'].append(X_train[i])\n",
    "    elif y_train[i] == 'mixed':\n",
    "        mixed['ICs'].append(X_train[i])\n",
    "    elif y_train[i] == 'lateyes':\n",
    "        lateyes['ICs'].append(X_train[i])\n",
    "    elif y_train[i] == 'heart':\n",
    "        heart['ICs'].append(X_train[i])\n",
    "    else:\n",
    "        raise ValueError('Unknown class label: ' + y_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "So Dr. B raised a really good point that I had forgotten. Sikmeans does not take just raw ICs, it takes random windows sampled from throughout the ICs. I need to better remember the details of the paper. Will reread it now, but also I need to rework the below and take windows from them first before passing to sikmeans.\n",
    "\n",
    "So the first argument to sikmeans should be of shape (# ICs, window length).\n",
    "\n",
    "His function where he takes window slices out of them is in the dataloaders file. A bit of odd organization and I looked right over it. Just have that in a new window below.\n",
    "\n",
    "He even says \"ICs from diff subjects have diff lengths so don't straightforwardly concatenate into an array\" and I missed that. My error.\n",
    "\n",
    "Taking windows below:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#grab windows from the ICs\n",
    "#not all ICs have the same length, so must take windows. This is what we pass in to sikmeans anyways. Different windows for each class.\n",
    "\n",
    "#what is the window len? Set hyperparams above. Later on will modify to be chosen from grid search in nested cross validation.\n",
    "window_len = 256 #what's the sampling rate again?\n",
    "\n",
    "windows_per_class = {'neural': [], 'blink': [], 'muscle': [], 'mixed': [], 'lateyes': [], 'heart': []}\n",
    "\n",
    "tot_num_windows = 0 #to house total number of windows for all ICs\n",
    "for label in all_classes:\n",
    "    #iterate through class and collect info about sequence lengths\n",
    "    ic_lengths = []\n",
    "    n_ics = len(label['ICs'])\n",
    "    for ic in label['ICs']:\n",
    "        ic_lengths.append(len(ic))\n",
    "\n",
    "    #Currently, assuming that we are not taking a subset of the ICs at all. Carlos had the option for that in his earlier window code.\n",
    "    #So the number of windows per ic will just be the length of each ic / win len.\n",
    "    n_windows_per_ic = [ic_len // window_len for ic_len in ic_lengths]\n",
    "    tot_num_windows += sum(n_windows_per_ic)\n",
    "\n",
    "    #Now that we have the number of windows per ic, we can create the windows.\n",
    "\n",
    "    rng = np.random.RandomState(42)\n",
    "\n",
    "    X = np.zeros((tot_num_windows, window_len)) #X is for each class. Stack later\n",
    "    win_start = 0\n",
    "    for label in all_classes:\n",
    "        for ic in label['ICs']:\n",
    "            windows_per_ic = len(ic) // window_len\n",
    "            time_idx = np.arange(0, len(ic)-window_len+1, window_len)\n",
    "            time_idx = rng.choice(time_idx, size=windows_per_ic, replace=False)\n",
    "            time_idx = time_idx[:, None] + np.arange(window_len)[None, :]\n",
    "            X[win_start:win_start+windows_per_ic] = ic[time_idx]\n",
    "            win_start += windows_per_ic\n",
    "\n",
    "    windows_per_class[label] = X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "X, calculated above, should now contain all windows from all ICs. Now we can pass this to sikmeans.\n",
    "Actually hold on. It needs to be done by class. Done. So now pass windows_per_class[label] to sikmeans in as the first parameter.\n",
    "\n",
    "Note - this may get to the point where I can't hold these in memory any longer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (210,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m rng \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m42\u001B[39m\u001B[38;5;66;03m#np.random.RandomState(42)\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m#need to do this per class.\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m neural[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcentroids\u001B[39m\u001B[38;5;124m'\u001B[39m], neural[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m], neural[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mshifts\u001B[39m\u001B[38;5;124m'\u001B[39m], neural[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdistances\u001B[39m\u001B[38;5;124m'\u001B[39m], neural[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minertia\u001B[39m\u001B[38;5;124m'\u001B[39m], _ \u001B[38;5;241m=\u001B[39m shift_invariant_k_means(\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mneural\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mICs\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m, num_clusters, centroid_len, metric\u001B[38;5;241m=\u001B[39mmetric, init\u001B[38;5;241m=\u001B[39minit, n_init\u001B[38;5;241m=\u001B[39mn_runs, rng\u001B[38;5;241m=\u001B[39mrng,  verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     12\u001B[0m blink[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcentroids\u001B[39m\u001B[38;5;124m'\u001B[39m], blink[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m], blink[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mshifts\u001B[39m\u001B[38;5;124m'\u001B[39m], blink[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdistances\u001B[39m\u001B[38;5;124m'\u001B[39m], blink[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minertia\u001B[39m\u001B[38;5;124m'\u001B[39m], _ \u001B[38;5;241m=\u001B[39m shift_invariant_k_means(blink[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mICs\u001B[39m\u001B[38;5;124m'\u001B[39m], num_clusters, centroid_len, metric\u001B[38;5;241m=\u001B[39mmetric, init\u001B[38;5;241m=\u001B[39minit, n_init\u001B[38;5;241m=\u001B[39mn_runs, rng\u001B[38;5;241m=\u001B[39mrng,  verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     14\u001B[0m muscle[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcentroids\u001B[39m\u001B[38;5;124m'\u001B[39m], muscle[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m], muscle[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mshifts\u001B[39m\u001B[38;5;124m'\u001B[39m], muscle[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdistances\u001B[39m\u001B[38;5;124m'\u001B[39m], muscle[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minertia\u001B[39m\u001B[38;5;124m'\u001B[39m], _ \u001B[38;5;241m=\u001B[39m shift_invariant_k_means(muscle[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mICs\u001B[39m\u001B[38;5;124m'\u001B[39m], num_clusters, centroid_len, metric\u001B[38;5;241m=\u001B[39mmetric, init\u001B[38;5;241m=\u001B[39minit, n_init\u001B[38;5;241m=\u001B[39mn_runs, rng\u001B[38;5;241m=\u001B[39mrng,  verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mValueError\u001B[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (210,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "from BOWaves.sikmeans.sikmeans_core import shift_invariant_k_means\n",
    "metric, init = 'cosine', 'random'\n",
    "num_clusters = 16\n",
    "centroid_len = 256\n",
    "n_runs = 3\n",
    "n_jobs = 1\n",
    "rng = 42#np.random.RandomState(42)\n",
    "\n",
    "#need to do this per class.\n",
    "neural['centroids'], neural['labels'], neural['shifts'], neural['distances'], neural['inertia'], _ = shift_invariant_k_means(windows_per_class['neural'], num_clusters, centroid_len, metric=metric, init=init, n_init=n_runs, rng=rng,  verbose=True)\n",
    "\n",
    "blink['centroids'], blink['labels'], blink['shifts'], blink['distances'], blink['inertia'], _ = shift_invariant_k_means(windows_per_class['blink'], num_clusters, centroid_len, metric=metric, init=init, n_init=n_runs, rng=rng,  verbose=True)\n",
    "\n",
    "muscle['centroids'], muscle['labels'], muscle['shifts'], muscle['distances'], muscle['inertia'], _ = shift_invariant_k_means(windows_per_class['muscle'], num_clusters, centroid_len, metric=metric, init=init, n_init=n_runs, rng=rng,  verbose=True)\n",
    "\n",
    "mixed['centroids'], mixed['labels'], mixed['shifts'], mixed['distances'], mixed['inertia'], _ = shift_invariant_k_means(windows_per_class['mixed'], num_clusters, centroid_len, metric=metric, init=init, n_init=n_runs, rng=rng,  verbose=True)\n",
    "\n",
    "lateyes['centroids'], lateyes['labels'], lateyes['shifts'], lateyes['distances'], lateyes['inertia'], _ = shift_invariant_k_means(windows_per_class['lateyes'], num_clusters, centroid_len, metric=metric, init=init, n_init=n_runs, rng=rng,  verbose=True)\n",
    "\n",
    "heart['centroids'], heart['labels'], heart['shifts'], heart['distances'], heart['inertia'], _ = shift_invariant_k_means(windows_per_class['heart'], num_clusters, centroid_len, metric=metric, init=init, n_init=n_runs, rng=rng,  verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now that we have the codebooks, let's run the bowav clf code.\n",
    "We want to do leave one subject out cross validation to try and classify the labels on the held out test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "First, define BOWav to create the bag-of-words representations of the features learned in the codebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sanity checks before continuing.\n",
    "\n",
    "windows_per_class, indexed by the labels, holds my codebooks.\n",
    "\n",
    "Each element should be of the shape (k, P) with k and P being the number of centroids and centroid length, respectively.\n",
    "\n",
    "Taken from Carlos' comment:\n",
    "        A sequence of codebooks. C[i].shape = (k, P), with k and P being the number of centroids and centroid lenght, respectively.\n",
    "\n",
    "The length of the centroids should be exactly equal to the window length, since each centroid is just a representative waveform. I.e., sikmeans just picks one of the data points to be most central, not that it's combining data points to get the mean of the prob distr or something.\n",
    "\n",
    "Mistake - the windows per class are not the centroids. Sikmeans returns a certain number of centroids. If they were the same for each that wouldn't make much sense. How many does it return? Whatever the num of clusters is - above I have set to 16.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#sanity check\n",
    "for label in windows_per_class:\n",
    "    print(label + ': ' + str(windows_per_class[label].shape))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from BOWaves.sikmeans.sikmeans_core import _assignment_step\n",
    "\n",
    "#X_train above should be my raw ICs.\n",
    "\n",
    "#does this work? forgot list concatenation syntax\n",
    "codebooks = neural['centroids'] + blink['centroids'] + muscle['centroids'] + mixed['centroids'] + lateyes['centroids'] + heart['centroids']\n",
    "\n",
    "def bag_of_waves(raw_ics, codebooks):\n",
    "    \"\"\"\n",
    "    Creates a bag-of-words representation of the input data using the codebooks.\n",
    "\n",
    "    The codebooks should be a concatenation of all the centroids learned above, one for each class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    codebooks - the array of centroids. I.e., neural['centroids']\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X: matrix of shape (n_ics, n_features)\n",
    "        The bag-of-words representation of the input data.\n",
    "        (note to self, this should definitely be a sparse matrix right?)\n",
    "    \"\"\"\n",
    "\n",
    "    #n_ics is the number of ICs from all classes in the codebooks list\n",
    "    n_ics = sum(len(codebook['ICs']) for codebook in codebooks)\n",
    "\n",
    "    #n_centroids is the number of centroids from all classes in the codebooks list\n",
    "    n_centroids = sum(len(codebook['centroids']) for codebook in codebooks)\n",
    "\n",
    "    x_squared_norms = None\n",
    "    X = np.zeros((n_ics, n_centroids), dtype=codebooks[0]['centroids'].dtype)\n",
    "\n",
    "    for ic in range(n_ics):\n",
    "        for centroid in range(n_centroids):\n",
    "            nu, _, _ = _assignment_step(codebooks[centroid]['ICs'][ic], codebooks[centroid]['centroids'], x_squared_norms, metric='cosine')\n",
    "\n",
    "            nu, counts = np.unique(nu, return_counts=True)\n",
    "\n",
    "            i_feature = nu + centroid * n_centroids\n",
    "            X[ic, i_feature] = counts\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have the bag-of-words representation, we can run the classifier.\n",
    "\n",
    "The above should all work, calculating codebooks and getting their bowav representation. The only problem that I foresee there besides the usual bugs are memory issues.\n",
    "\n",
    "So how will I do the below? Use Carlos' code as a base. I need to get the subject indices for LOO. Then do grid search within that. But how can I get the subject if we're combining all the ICs into their different classes?\n",
    "I could put in raw ICs from each subject in the bag_of_waves function. That'll get a per subject bowav representation, built on codebooks learned from all subjects. That should probably work. But, check the ICLR paper again to make sure."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut, grid_search_cv #not sure if this is right\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "#list of classifier hyperparameters\n",
    "\n",
    "\n",
    "#get data here - want raw_ICs, codebooks. Split between subjects.\n",
    "#note - should I make a new function for it as Carlos did?\n",
    "\n",
    "\n",
    "#pipeline set up, params just what Carlos used\n",
    "pipe = Pipeline([('scaler', TfidfTransformer()), ('clf', LogisticRegression())])\n",
    "\n",
    "clf_params = dict(\n",
    "    clf__class_weight='balanced',\n",
    "    clf__solver='saga',\n",
    "    clf__penalty='elasticnet', #Carlos' default. could also be none, l1, or l2\n",
    "    clf__random_state=np.random.RandomState(13),\n",
    "    clf__multi_class='multinomial',\n",
    "    clf__warm_start=True,\n",
    "    clf__max_iter=1000, #Carlos' default.\n",
    ")\n",
    "pipe.set_params(**clf_params)\n",
    "\n",
    "candidate_params = dict(\n",
    "    clf__C=[0.1,1,10], #regularization factor\n",
    "    clf__l1_ratio=[0, 0.2, 0.4, 0.6, 0.8, 1],\n",
    ")\n",
    "\n",
    "results = grid_search_cv(\n",
    "    pipe,\n",
    "    candidate_params,\n",
    "    X,\n",
    "    y,\n",
    "    cv,\n",
    "    n_jobs=1 #parallelization\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
