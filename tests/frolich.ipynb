{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "This notebook is for running a first try of BOWave on the Frolich et. al data to see if we match their paper's results.\n",
    "This requires 16 < x < 32 gb of RAM. Recommend running on Caviness with --mem-per-cpu=32gb flag set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#Load ICs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Obtaining dependency information for matplotlib from https://files.pythonhosted.org/packages/c2/da/a5622266952ab05dc3995d77689cba600e49ea9d6c51d469c077695cb719/matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Obtaining dependency information for contourpy>=1.0.1 from https://files.pythonhosted.org/packages/aa/55/02c6d24804592b862b38a85c9b3283edc245081390a520ccd11697b6b24f/contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Obtaining dependency information for fonttools>=4.22.0 from https://files.pythonhosted.org/packages/2b/e8/61b8525acf26ec222518bdff127ae502bfa3408981fb5e5493f2b037d7fb/fonttools-4.42.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading fonttools-4.42.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (150 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.0/151.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Obtaining dependency information for kiwisolver>=1.0.1 from https://files.pythonhosted.org/packages/6f/40/4ab1fdb57fced80ce5903f04ae1aed7c1d5939dda4fd0c0aa526c12fe28a/kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in /work/cniel/ajmeek/BOWaves/venv/lib/python3.10/site-packages (from matplotlib) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /work/cniel/ajmeek/BOWaves/venv/lib/python3.10/site-packages (from matplotlib) (23.1)\n",
      "Collecting pillow>=6.2.0 (from matplotlib)\n",
      "  Obtaining dependency information for pillow>=6.2.0 from https://files.pythonhosted.org/packages/7b/c9/08de9a629ce7cdeaea0ddca716e9efcd1844b2650f5b9dd8ec5609e40ffe/Pillow-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading Pillow-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting pyparsing<3.1,>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /work/cniel/ajmeek/BOWaves/venv/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /work/cniel/ajmeek/BOWaves/venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.7/300.7 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fonttools-4.42.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Pillow-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.1.0 cycler-0.11.0 fonttools-4.42.1 kiwisolver-1.4.5 matplotlib-3.7.2 pillow-10.0.0 pyparsing-3.0.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /work/cniel/ajmeek/BOWaves/venv/lib/python3.10/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /work/cniel/ajmeek/BOWaves/venv/lib/python3.10/site-packages (from scikit-learn) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /work/cniel/ajmeek/BOWaves/venv/lib/python3.10/site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /work/cniel/ajmeek/BOWaves/venv/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /work/cniel/ajmeek/BOWaves/venv/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pyrootutils\n",
    "\n",
    "pyrootutils.set_root(path='/work/cniel/ajmeek/BOWaves/BOWaves', pythonpath=True)\n",
    "#path = pyrootutils.find_root(search_from=__file__, indicator=\".git\")\n",
    "#pyrootutils.set_root(path=path, pythonpath=True)\n",
    "\n",
    "import BOWaves.utilities.dataloaders as dataloaders\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#frolich_ics = {'ICs': np.array([]), 'labels': np.array([])}\n",
    "frolich_ics = {'ICs': [], 'labels': []}\n",
    "\n",
    "#for file in directory frolich data\n",
    "frolich_data = os.listdir('../data/frolich')\n",
    "\n",
    "#filter out subdirectories such as /img\n",
    "frolich_data = [file for file in frolich_data if not os.path.isdir(file)]\n",
    "\n",
    "for file in frolich_data:\n",
    "    ICs, labels = dataloaders.load_and_visualize_mat_file_frolich('../data/frolich/' + file, visualize=False)\n",
    "    frolich_ics['ICs'].extend(ICs)\n",
    "    frolich_ics['labels'].extend(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now create codebooks since we have the ICs and their labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "First split off 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(frolich_ics['ICs'], frolich_ics['labels'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now out of the training set, split into the different classes. Frolich's data has 4 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if len(X_train) != len(y_train):\n",
    "    raise ValueError('X_train and y_train are not the same length.')\n",
    "\n",
    "# Forgot what the classes were. check on Caviness\n",
    "neural = {'ICs': [], 'centroids': [], 'labels': [], 'shifts': [], 'distances': [], 'inertia': []}\n",
    "blink = {'ICs': [], 'centroids': [], 'labels': [], 'shifts': [], 'distances': [], 'inertia': []}\n",
    "muscle = {'ICs': [], 'centroids': [], 'labels': [], 'shifts': [], 'distances': [], 'inertia': []}\n",
    "mixed = {'ICs': [], 'centroids': [], 'labels': [], 'shifts': [], 'distances': [], 'inertia': []}\n",
    "lateyes = {'ICs': [], 'centroids': [], 'labels': [], 'shifts': [], 'distances': [], 'inertia': []}\n",
    "heart = {'ICs': [], 'centroids': [], 'labels': [], 'shifts': [], 'distances': [], 'inertia': []}\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    if y_train[i] == 'neural':\n",
    "        neural['ICs'].append(X_train[i])\n",
    "    elif y_train[i] == 'blink':\n",
    "        blink['ICs'].append(X_train[i])\n",
    "    elif y_train[i] == 'muscle':\n",
    "        muscle['ICs'].append(X_train[i])\n",
    "    elif y_train[i] == 'mixed':\n",
    "        mixed['ICs'].append(X_train[i])\n",
    "    elif y_train[i] == 'lateyes':\n",
    "        lateyes['ICs'].append(X_train[i])\n",
    "    elif y_train[i] == 'heart':\n",
    "        heart['ICs'].append(X_train[i])\n",
    "    else:\n",
    "        raise ValueError('Unknown class label: ' + y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (210,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m rng \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m\u001b[38;5;66;03m#np.random.RandomState(42)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#need to do this per class.\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m neural[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcentroids\u001b[39m\u001b[38;5;124m'\u001b[39m], neural[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m], neural[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshifts\u001b[39m\u001b[38;5;124m'\u001b[39m], neural[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistances\u001b[39m\u001b[38;5;124m'\u001b[39m], neural[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minertia\u001b[39m\u001b[38;5;124m'\u001b[39m], _ \u001b[38;5;241m=\u001b[39m shift_invariant_k_means(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneural\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mICs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, num_clusters, centroid_len, metric\u001b[38;5;241m=\u001b[39mmetric, init\u001b[38;5;241m=\u001b[39minit, n_init\u001b[38;5;241m=\u001b[39mn_runs, rng\u001b[38;5;241m=\u001b[39mrng,  verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m blink[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcentroids\u001b[39m\u001b[38;5;124m'\u001b[39m], blink[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m], blink[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshifts\u001b[39m\u001b[38;5;124m'\u001b[39m], blink[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistances\u001b[39m\u001b[38;5;124m'\u001b[39m], blink[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minertia\u001b[39m\u001b[38;5;124m'\u001b[39m], _ \u001b[38;5;241m=\u001b[39m shift_invariant_k_means(blink[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mICs\u001b[39m\u001b[38;5;124m'\u001b[39m], num_clusters, centroid_len, metric\u001b[38;5;241m=\u001b[39mmetric, init\u001b[38;5;241m=\u001b[39minit, n_init\u001b[38;5;241m=\u001b[39mn_runs, rng\u001b[38;5;241m=\u001b[39mrng,  verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m muscle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcentroids\u001b[39m\u001b[38;5;124m'\u001b[39m], muscle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m], muscle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshifts\u001b[39m\u001b[38;5;124m'\u001b[39m], muscle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistances\u001b[39m\u001b[38;5;124m'\u001b[39m], muscle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minertia\u001b[39m\u001b[38;5;124m'\u001b[39m], _ \u001b[38;5;241m=\u001b[39m shift_invariant_k_means(muscle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mICs\u001b[39m\u001b[38;5;124m'\u001b[39m], num_clusters, centroid_len, metric\u001b[38;5;241m=\u001b[39mmetric, init\u001b[38;5;241m=\u001b[39minit, n_init\u001b[38;5;241m=\u001b[39mn_runs, rng\u001b[38;5;241m=\u001b[39mrng,  verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (210,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "from BOWaves.sikmeans.sikmeans_core import shift_invariant_k_means\n",
    "metric, init = 'cosine', 'random'\n",
    "num_clusters = 16\n",
    "centroid_len = 256\n",
    "n_runs = 3\n",
    "n_jobs = 1\n",
    "rng = 42#np.random.RandomState(42)\n",
    "\n",
    "#need to do this per class.\n",
    "neural['centroids'], neural['labels'], neural['shifts'], neural['distances'], neural['inertia'], _ = shift_invariant_k_means(np.asarray(neural['ICs']), num_clusters, centroid_len, metric=metric, init=init, n_init=n_runs, rng=rng,  verbose=True)\n",
    "\n",
    "blink['centroids'], blink['labels'], blink['shifts'], blink['distances'], blink['inertia'], _ = shift_invariant_k_means(blink['ICs'], num_clusters, centroid_len, metric=metric, init=init, n_init=n_runs, rng=rng,  verbose=True)\n",
    "\n",
    "muscle['centroids'], muscle['labels'], muscle['shifts'], muscle['distances'], muscle['inertia'], _ = shift_invariant_k_means(muscle['ICs'], num_clusters, centroid_len, metric=metric, init=init, n_init=n_runs, rng=rng,  verbose=True)\n",
    "\n",
    "mixed['centroids'], mixed['labels'], mixed['shifts'], mixed['distances'], mixed['inertia'], _ = shift_invariant_k_means(mixed['ICs'], num_clusters, centroid_len, metric=metric, init=init, n_init=n_runs, rng=rng,  verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now that we have the codebooks, let's run the bowav clf code.\n",
    "We want to do leave one subject out cross validation to try and classify the labels on the held out test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "First, define BOWav to create the bag-of-words representations of the features learned in the codebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from BOWaves.sikmeans.sikmeans_core import _assignment_step\n",
    "\n",
    "def bag_of_waves(codebooks):\n",
    "    \"\"\"\n",
    "    Creates a bag-of-words representation of the input data using the codebooks.\n",
    "\n",
    "    The codebooks are a list of dictionaries, where each dictionary contains the centroids, labels, shifts, distances, and raw ICs of a codebook. Therefore, they're the only thing we need to pass in.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    codebooks\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X: matrix of shape (n_ics, n_features)\n",
    "        The bag-of-words representation of the input data.\n",
    "    \"\"\"\n",
    "\n",
    "    #n_ics is the number of ICs from all classes in the codebooks list\n",
    "    n_ics = sum(len(codebook['ICs']) for codebook in codebooks)\n",
    "\n",
    "    #n_centroids is the number of centroids from all classes in the codebooks list\n",
    "    n_centroids = sum(len(codebook['centroids']) for codebook in codebooks)\n",
    "\n",
    "    x_squared_norms = None\n",
    "    X = np.zeros((n_ics, n_centroids), dtype=codebooks[0]['centroids'].dtype)\n",
    "\n",
    "    for ic in range(n_ics):\n",
    "        for centroid in range(n_centroids):\n",
    "            nu, _, _ = _assignment_step(codebooks[centroid]['ICs'][ic], codebooks[centroid]['centroids'], x_squared_norms, metric='cosine')\n",
    "\n",
    "            nu, counts = np.unique(nu, return_counts=True)\n",
    "\n",
    "            i_feature = nu + centroid * n_centroids\n",
    "            X[ic, i_feature] = counts\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
